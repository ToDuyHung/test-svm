# -*- coding: utf-8 -*-
"""SVM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KeQ6AE_VcHS7m3lpLnKJYOgeazuzavmR

# SVM Exercises

- dataset: 'multi-intent.csv' for multi-class SVM + TF-IDF
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import pickle
import re

# %matplotlib inline

"""# **MULTI CLASS SVM + TF_IDF**"""

df = pd.read_csv('multi-intent.csv')

"""## 1) Data Analysis

Drop the index column
"""

df = df.drop(columns=['Unnamed: 0'])

df.head()

df.columns

df.isnull().sum()

df.dtypes

df.shape

df['intent'].value_counts()

"""## 2) Data Processing

Determine features and labels
"""

X = df['text'].values
y = df['intent'].values

"""Encoding (Lable Encoding)"""

from sklearn.feature_extraction.text import TfidfVectorizer

# vectorizer = TfidfVectorizer()
tfidfconverter = pickle.load(open('/content/tfidf.pickle', 'rb'))

def preprocess(doc, rm_short=False):
    results = []
    for i in range(len(doc)):  
        result = str(doc[i]).lower()

        result = re.sub('^khÃ¡ch.*:', '', result)
        result = re.sub('\W', ' ', str(result))
        result = re.sub('\s+', ' ', result, flags=re.I)
        result = re.sub('^\s|\s$', '', result)
        result = re.sub('.*: ', '', result)

        if len(result) > 2 or rm_short is False:
            results.append(result)
    return results

full = pd.read_csv('/content/Output2.txt', sep='\n', header=None)
full = full.dropna()
X_corp = full.values
corpus = preprocess(X_corp, rm_short=True)

X = tfidfconverter.transform(df["text"]).toarray()

# from sklearn.preprocessing import LabelEncoder
# le = LabelEncoder()
# y = le.fit_transform(y)

"""### Data Standardization"""

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X)
X = scaler.transform(X)

"""### Spliting dataset into training set and test set"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

print(X_train.shape, y_train.size)
print(X_test.shape, y_test.shape)

"""## 3) Support Vector Machine

*   Using differrent kernels

### Default kernel
"""

from sklearn.svm import SVC
from sklearn import metrics
svc=SVC() #Default hyperparameters

svc

"""=> kernel='rbf' is the default"""

svc.fit(X_train,y_train)

y_pred=svc.predict(X_test)
print('Accuracy Score:')
print(metrics.accuracy_score(y_test,y_pred))

"""### Linear kernel"""

svc=SVC(kernel='linear')

svc

svc.fit(X_train,y_train)
y_pred=svc.predict(X_test)
print('Accuracy Score:')
print(metrics.accuracy_score(y_test,y_pred))

"""### Polynomial kernel"""

svc=SVC(kernel='poly')

svc

svc.fit(X_train,y_train)
y_pred=svc.predict(X_test)
print('Accuracy Score:')
print(metrics.accuracy_score(y_test,y_pred))

"""## 4) K-fold cross validation with different kernels

### a) CV on Linear kernel
"""

from sklearn.model_selection import cross_val_score
svc=SVC(kernel='linear')
svc

"""=> C = 1.0"""

scores = cross_val_score(svc, X, y, cv=10, scoring='accuracy') #cv is cross validation
print(scores)

print(scores.mean())

"""####Change C value"""

print("Change C vaue")

C_range=list(range(1,56))
acc_score=[]

for c in C_range:
    svc = SVC(kernel='linear', C=c)
    scores = cross_val_score(svc, X, y, cv=10, scoring='accuracy')
    acc_score.append(scores.mean())
print(acc_score)

import matplotlib.pyplot as plt

C_values=list(range(1,56))
# plot the value of C for SVM (x-axis) versus the cross-validated accuracy (y-axis)
plt.plot(C_values,acc_score)
plt.xticks(np.arange(0,57,2))
plt.xlabel('Value of C for SVC')
plt.ylabel('Cross-Validated Accuracy')

